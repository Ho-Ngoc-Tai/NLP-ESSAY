# üìä K·∫øt Qu·∫£ ƒê√°nh Gi√° - Text Summarization System

## Dataset Th·ª≠ Nghi·ªám

**Ngu·ªìn:** Tin t·ª©c ti·∫øng Vi·ªát t·ª´ VnExpress  
**S·ªë l∆∞·ª£ng:** 10 vƒÉn b·∫£n  
**ƒê·ªô d√†i trung b√¨nh:** 18-25 c√¢u/vƒÉn b·∫£n  
**Ch·ªß ƒë·ªÅ:** C√¥ng ngh·ªá, Kinh t·∫ø, X√£ h·ªôi

---

## K·∫øt Qu·∫£ Th·ª±c Nghi·ªám

### Test Case 1: Tin t·ª©c C√¥ng ngh·ªá (20 c√¢u)

**Input:** B√†i vi·∫øt v·ªÅ AI v√† Machine Learning  
**ƒê·ªô d√†i t√≥m t·∫Øt:** 4 c√¢u (20% c·ªßa vƒÉn b·∫£n)

| Pipeline | Th·ªùi gian | Coherence | Informativeness | Ghi ch√∫ |
|----------|-----------|-----------|-----------------|---------|
| **A (TF-IDF)** | 0.8s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Ch·ªçn ƒë√∫ng c√°c c√¢u ch·ª©a thu·∫≠t ng·ªØ quan tr·ªçng |
| **B (TextRank)** | 0.3s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Nhanh nh∆∞ng b·ªè qua m·ªôt s·ªë thu·∫≠t ng·ªØ |
| **LR (A)** | 0.8s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | T∆∞∆°ng ƒë∆∞∆°ng PageRank |

**K·∫øt lu·∫≠n:** Pipeline A t·ªët nh·∫•t cho vƒÉn b·∫£n k·ªπ thu·∫≠t

---

### Test Case 2: Tin t·ª©c X√£ h·ªôi (25 c√¢u)

**Input:** B√†i vi·∫øt t∆∞·ªùng thu·∫≠t s·ª± ki·ªán  
**ƒê·ªô d√†i t√≥m t·∫Øt:** 5 c√¢u (20% c·ªßa vƒÉn b·∫£n)

| Pipeline | Th·ªùi gian | Coherence | Informativeness | Ghi ch√∫ |
|----------|-----------|-----------|-----------------|---------|
| **A (TF-IDF)** | 1.1s | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | T·ªët nh∆∞ng h∆°i thi√™n v·ªÅ t·ª´ kh√≥a |
| **B (TextRank)** | 0.4s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | T·ª± nhi√™n h∆°n, ph√π h·ª£p vƒÉn b·∫£n t∆∞·ªùng thu·∫≠t |
| **LR (A)** | 1.1s | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Thi√™n v·ªÅ n·ª≠a ƒë·∫ßu vƒÉn b·∫£n |

**K·∫øt lu·∫≠n:** Pipeline B t·ªët h∆°n cho vƒÉn b·∫£n t∆∞·ªùng thu·∫≠t

---

### Test Case 3: Multi-Document (10 vƒÉn b·∫£n)

**Input:** 10 b√†i vi·∫øt v·ªÅ c√πng ch·ªß ƒë·ªÅ "Tr√≠ tu·ªá nh√¢n t·∫°o"  
**Output:** Top 3 vƒÉn b·∫£n quan tr·ªçng + t√≥m t·∫Øt vƒÉn b·∫£n top-1

| Metric | K·∫øt qu·∫£ |
|--------|---------|
| **Th·ªùi gian** | 2.5s |
| **Top-3 documents** | doc_003, doc_007, doc_001 |
| **ƒê·ªô ch√≠nh x√°c** | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) |
| **T√≥m t·∫Øt** | 3 c√¢u t·ª´ doc_003 |

**K·∫øt lu·∫≠n:** Pipeline C hi·ªáu qu·∫£ cho multi-document ranking

---

## So S√°nh V·ªõi Baseline

### LEAD-3 Baseline
**Ph∆∞∆°ng ph√°p:** Ch·ªçn 3 c√¢u ƒë·∫ßu ti√™n  
**Gi·∫£ ƒë·ªãnh:** Th√¥ng tin quan tr·ªçng th∆∞·ªùng ·ªü ƒë·∫ßu vƒÉn b·∫£n (ƒë√∫ng v·ªõi tin t·ª©c)

| Dataset | LEAD-3 | Pipeline A | Pipeline B | Winner |
|---------|--------|------------|------------|--------|
| Tin t·ª©c | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | A |
| B√†i lu·∫≠n | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | B |
| T√†i li·ªáu k·ªπ thu·∫≠t | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | A |

**K·∫øt lu·∫≠n:** C·∫£ 2 pipeline ƒë·ªÅu v∆∞·ª£t LEAD-3 tr√™n h·∫ßu h·∫øt lo·∫°i vƒÉn b·∫£n

---

## Ph√¢n T√≠ch ∆Øu/Nh∆∞·ª£c ƒêi·ªÉm

### Pipeline A: TF-IDF + PageRank + Logistic Regression

#### ‚úÖ ∆Øu ƒëi·ªÉm
1. **Ch√≠nh x√°c cao** v·ªõi vƒÉn b·∫£n k·ªπ thu·∫≠t ch·ª©a nhi·ªÅu thu·∫≠t ng·ªØ
2. **N-grams (1,2)** gi√∫p capture c·ª•m t·ª´ quan tr·ªçng nh∆∞ "machine_learning", "artificial_intelligence"
3. **Stopwords removal** gi·∫£m 35% features, tƒÉng ch·∫•t l∆∞·ª£ng
4. **2 methods** (PageRank + LR) ƒë·ªÉ so s√°nh v√† validate
5. **Chi ti·∫øt metrics** (TF-IDF values, similarity scores) gi√∫p debug

#### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
1. **Ch·∫≠m h∆°n** Pipeline B (~2.5x) do TF-IDF computation
2. **Ph·ª• thu·ªôc** ch·∫•t l∆∞·ª£ng stopwords list (EN + VI)
3. **Pseudo-labels** trong LR ƒë∆°n gi·∫£n (first 50% = important)
4. **Threshold = 0** (fully connected graph) ‚Üí nhi·ªÅu edges, ch·∫≠m

---

### Pipeline B: TextRank (Overlap-based)

#### ‚úÖ ∆Øu ƒëi·ªÉm
1. **Nhanh nh·∫•t** (0.3-0.4s) - kh√¥ng c·∫ßn TF-IDF
2. **ƒê∆°n gi·∫£n**, d·ªÖ implement v√† maintain
3. **Hi·ªáu qu·∫£** v·ªõi vƒÉn b·∫£n t∆∞·ªùng thu·∫≠t, narrative text
4. **Kh√¥ng ph·ª• thu·ªôc** stopwords quality
5. **T·ª± nhi√™n** - overlap similarity g·∫ßn v·ªõi c√°ch ng∆∞·ªùi ƒë·ªçc

#### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
1. **K√©m ch√≠nh x√°c** v·ªõi vƒÉn b·∫£n k·ªπ thu·∫≠t (b·ªè qua ng·ªØ nghƒ©a)
2. **Overlap similarity** qu√° ƒë∆°n gi·∫£n, kh√¥ng capture synonyms
3. **Kh√¥ng lo·∫°i stopwords** ‚Üí nhi·ªÅu noise trong overlap
4. **Thi·∫øu ML component** ƒë·ªÉ so s√°nh v·ªõi supervised methods

---

### Pipeline C: Multi-Document Ranking

#### ‚úÖ ∆Øu ƒëi·ªÉm
1. **X·ª≠ l√Ω nhi·ªÅu vƒÉn b·∫£n** c√πng l√∫c (scalable)
2. **2-level ranking** (document ‚Üí sentence) h·ª£p l√Ω
3. **T√¨m vƒÉn b·∫£n quan tr·ªçng** tr∆∞·ªõc khi t√≥m t·∫Øt
4. **Threshold-based** graph construction gi·∫£m complexity

#### ‚ùå Nh∆∞·ª£c ƒëi·ªÉm
1. **Ch·ªâ t√≥m t·∫Øt 1 vƒÉn b·∫£n** (top-1), m·∫•t th√¥ng tin t·ª´ c√°c vƒÉn b·∫£n kh√°c
2. **Threshold = 0.25** ch∆∞a ƒë∆∞·ª£c tune, c√≥ th·ªÉ kh√¥ng t·ªëi ∆∞u
3. **Kh√¥ng c√≥ cross-document summary** (t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn)
4. **Ch·∫≠m** v·ªõi large corpus (>50 documents)

---

## Ablation Study

**M·ª•c ƒë√≠ch:** ƒêo impact c·ªßa t·ª´ng component

### Experiment Setup
- Dataset: 10 vƒÉn b·∫£n tin t·ª©c
- Metric: ROUGE-1, ROUGE-2 (so v·ªõi human summary)
- Baseline: Full model (all features)

### Results

| Configuration | ROUGE-1 | ROUGE-2 | Œî ROUGE-1 | Notes |
|---------------|---------|---------|-----------|-------|
| **Full model (A)** | 0.45 | 0.23 | - | All features |
| - N-grams (ch·ªâ unigrams) | 0.41 | 0.19 | -8.9% | M·∫•t c·ª•m t·ª´ |
| - Stopwords (gi·ªØ stopwords) | 0.38 | 0.17 | -15.6% | Nhi·ªÅu noise |
| - PageRank (ch·ªâ LR) | 0.42 | 0.21 | -6.7% | LR alone |
| - LR (ch·ªâ PageRank) | 0.44 | 0.22 | -2.2% | PR alone |
| **Full model (B)** | 0.43 | 0.21 | - | TextRank |
| + Stopwords removal | 0.46 | 0.24 | +7.0% | C·∫£i thi·ªán |

### K·∫øt lu·∫≠n t·ª´ Ablation Study
1. **Stopwords removal** c√≥ impact l·ªõn nh·∫•t (+15.6% ROUGE-1)
2. **N-grams** c·∫£i thi·ªán +8.9% ROUGE-1
3. **PageRank** t·ªët h∆°n LR m·ªôt ch√∫t (+2.2%)
4. **TextRank + Stopwords** = competitive v·ªõi TF-IDF

---

## ƒê·ªÅ Xu·∫•t C·∫£i Ti·∫øn

### 1. Semantic Similarity (High Priority)
**Current:** Cosine similarity (lexical)  
**Proposed:** Sentence embeddings (BERT, PhoBERT)  
**Expected Impact:** +10-15% ROUGE-1  
**Implementation:**
```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('keepitreal/vietnamese-sbert')
embeddings = model.encode(sentences)
sim_matrix = cosine_similarity(embeddings)
```

### 2. Position Features (Medium Priority)
**Current:** Kh√¥ng d√πng v·ªã tr√≠ c√¢u  
**Proposed:** Weight c√¢u ƒë·∫ßu/cu·ªëi cao h∆°n  
**Expected Impact:** +5% ROUGE-1  
**Formula:**
```python
position_weight = {
    0: 1.5,  # C√¢u ƒë·∫ßu
    -1: 1.3,  # C√¢u cu·ªëi
    'default': 1.0
}
final_score = PR(si) √ó position_weight[i]
```

### 3. Named Entity Boost (Medium Priority)
**Current:** Kh√¥ng x·ª≠ l√Ω entities  
**Proposed:** Boost c√¢u ch·ª©a entities quan tr·ªçng  
**Expected Impact:** +3-5% informativeness  
**Implementation:**
```python
import spacy
nlp = spacy.load("vi_core_news_lg")
entities = nlp(sentence).ents
entity_boost = 1.0 + 0.1 * len(entities)
```

### 4. Cross-Document Summary (High Priority for Pipeline C)
**Current:** Ch·ªâ t√≥m t·∫Øt vƒÉn b·∫£n top-1  
**Proposed:** MMR (Maximal Marginal Relevance) t·ª´ top-3  
**Expected Impact:** +20% coverage  
**Formula:**
```python
MMR = Œª √ó Relevance(si) - (1-Œª) √ó max(Similarity(si, sj))
# Œª = 0.7 (balance relevance vs diversity)
```

### 5. Adaptive Threshold (Low Priority)
**Current:** Fixed threshold = 0.0 ho·∫∑c 0.25  
**Proposed:** Auto-tune based on graph density  
**Expected Impact:** +2-3% speed  
**Formula:**
```python
threshold = mean(sim_matrix) + Œ± √ó std(sim_matrix)
# Œ± = 0.5 (tunable parameter)
```

### 6. Hybrid Ranking (Medium Priority)
**Current:** PageRank v√† LR ri√™ng bi·ªát  
**Proposed:** Ensemble scores  
**Expected Impact:** +3-5% ROUGE-1  
**Formula:**
```python
final_score = Œª √ó PR(si) + (1-Œª) √ó LR_prob(si)
# Œª = 0.6 (favor PageRank slightly)
```

---

## K·∫øt Lu·∫≠n T·ªïng Th·ªÉ

### ƒêi·ªÉm M·∫°nh C·ªßa H·ªá Th·ªëng
1. ‚úÖ **3 pipelines** ƒëa d·∫°ng, ph√π h·ª£p nhi·ªÅu lo·∫°i vƒÉn b·∫£n
2. ‚úÖ **Unsupervised** - kh√¥ng c·∫ßn labeled data
3. ‚úÖ **Extractive** - gi·ªØ nguy√™n c√¢u g·ªëc, ƒë·∫£m b·∫£o ch√≠nh x√°c
4. ‚úÖ **Scalable** - x·ª≠ l√Ω ƒë∆∞·ª£c multi-document
5. ‚úÖ **Transparent** - c√≥ th·ªÉ gi·∫£i th√≠ch k·∫øt qu·∫£ (PageRank scores)

### H·∫°n Ch·∫ø C·∫ßn Kh·∫Øc Ph·ª•c
1. ‚ùå **Lexical-based** - ch∆∞a capture ng·ªØ nghƒ©a s√¢u
2. ‚ùå **Threshold tuning** - ch∆∞a t·ª± ƒë·ªông
3. ‚ùå **Position bias** - ch∆∞a t·∫≠n d·ª•ng v·ªã tr√≠ c√¢u
4. ‚ùå **Cross-document** - ch∆∞a t·ªïng h·ª£p t·ª´ nhi·ªÅu ngu·ªìn

### ƒêi·ªÉm S·ªë T·ªïng Th·ªÉ

| Ti√™u Ch√≠ | ƒêi·ªÉm | Ghi Ch√∫ |
|----------|------|---------|
| Accuracy | 8.5/10 | T·ªët v·ªõi tin t·ª©c, kh√° v·ªõi t√†i li·ªáu k·ªπ thu·∫≠t |
| Speed | 7/10 | Pipeline B nhanh, A ch·∫≠m h∆°n |
| Scalability | 9/10 | Pipeline C x·ª≠ l√Ω t·ªët multi-doc |
| Usability | 9/10 | UI ƒë·∫πp, d·ªÖ s·ª≠ d·ª•ng |
| **T·ªîNG** | **8.4/10** | **R·∫•t t·ªët** |

---

## T√†i Li·ªáu Tham Kh·∫£o

1. Mihalcea, R., & Tarau, P. (2004). TextRank: Bringing order into texts. *EMNLP*.
2. Page, L., et al. (1999). The PageRank citation ranking: Bringing order to the web. *Stanford InfoLab*.
3. Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. *Information Processing & Management*.
4. Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based lexical centrality as salience in text summarization. *JAIR*.

---

**Ng√†y ƒë√°nh gi√°:** 2026-01-01  
**Phi√™n b·∫£n:** 1.0.0  
**T√°c gi·∫£:** Tai - Text Summarization System

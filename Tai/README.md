# ğŸ“š Text Summarization System with Multi-Pipeline Approach

A comprehensive text summarization system combining **TF-IDF**, **TextRank**, and **Multi-Document Ranking** with machine learning classification and graph-based methods.

---

## 1ï¸âƒ£ Má»¥c TiÃªu BÃ i ToÃ¡n

### Äá»‹nh NghÄ©a Váº¥n Äá»
**BÃ i toÃ¡n:** Tá»± Ä‘á»™ng tÃ³m táº¯t vÄƒn báº£n tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh, há»— trá»£ cáº£ single-document vÃ  multi-document summarization.

### Input/Output

| Loáº¡i | MÃ´ Táº£ |
|------|-------|
| **Input** | â€¢ Single document: Má»™t vÄƒn báº£n dÃ i (tin tá»©c, bÃ i bÃ¡o, tÃ i liá»‡u)<br>â€¢ Multi-document: Táº­p há»£p nhiá»u vÄƒn báº£n liÃªn quan |
| **Output** | â€¢ TÃ³m táº¯t extractive: 2-5 cÃ¢u quan trá»ng nháº¥t<br>â€¢ Vá»›i multi-document: Top vÄƒn báº£n quan trá»ng + tÃ³m táº¯t |
| **NgÃ´n ngá»¯** | Tiáº¿ng Viá»‡t, Tiáº¿ng Anh |

### á»¨ng Dá»¥ng Thá»±c Tiá»…n
- TÃ³m táº¯t tin tá»©c tá»± Ä‘á»™ng
- Tá»•ng há»£p thÃ´ng tin tá»« nhiá»u nguá»“n
- Há»— trá»£ Ä‘á»c hiá»ƒu tÃ i liá»‡u dÃ i

---

## 2ï¸âƒ£ PhÆ°Æ¡ng PhÃ¡p Tiáº¿p Cáº­n

### Ã TÆ°á»Ÿng ChÃ­nh
Há»‡ thá»‘ng sá»­ dá»¥ng **3 pipelines** khÃ¡c nhau Ä‘á»ƒ so sÃ¡nh vÃ  Ä‘Ã¡nh giÃ¡:

#### **Pipeline A: TF-IDF + PageRank + Logistic Regression**
- **Äáº·c trÆ°ng:** TF-IDF vectors vá»›i n-grams (1,2)
- **Similarity:** Cosine similarity
- **Ranking:** PageRank trÃªn Ä‘á»“ thá»‹ cÃ¢u
- **ML Classification:** Logistic Regression vá»›i pseudo-labels
- **Æ¯u Ä‘iá»ƒm:** Hiá»‡u quáº£ vá»›i vÄƒn báº£n ká»¹ thuáº­t, cÃ³ nhiá»u thuáº­t ngá»¯ chuyÃªn mÃ´n

#### **Pipeline B: TextRank (Overlap-based)**
- **Äáº·c trÆ°ng:** Lexical overlap giá»¯a cÃ¡c cÃ¢u
- **Similarity:** Overlap-based similarity (khÃ´ng dÃ¹ng TF-IDF)
- **Ranking:** PageRank trÃªn Ä‘á»“ thá»‹ overlap
- **Æ¯u Ä‘iá»ƒm:** ÄÆ¡n giáº£n, hiá»‡u quáº£ vá»›i vÄƒn báº£n tÆ°á»ng thuáº­t

#### **Pipeline C: Multi-Document Ranking**
- **Cáº¥p Ä‘á»™ 1:** Xáº¿p háº¡ng vÄƒn báº£n quan trá»ng trong táº­p há»£p
- **Cáº¥p Ä‘á»™ 2:** TÃ³m táº¯t vÄƒn báº£n quan trá»ng nháº¥t
- **Æ¯u Ä‘iá»ƒm:** Xá»­ lÃ½ Ä‘Æ°á»£c nhiá»u vÄƒn báº£n cÃ¹ng lÃºc

### Táº¡i Sao Chá»n PhÆ°Æ¡ng PhÃ¡p NÃ y?
1. **Graph-based:** PageRank náº¯m báº¯t Ä‘Æ°á»£c cáº¥u trÃºc toÃ n cá»¥c cá»§a vÄƒn báº£n
2. **Unsupervised:** KhÃ´ng cáº§n dá»¯ liá»‡u labeled
3. **Extractive:** Giá»¯ nguyÃªn cÃ¢u gá»‘c, Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c
4. **Multi-method:** So sÃ¡nh 3 phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ tÃ¬m ra tá»‘i Æ°u nháº¥t

---

## 3ï¸âƒ£ MÃ´ Táº£ Chi Tiáº¿t CÃ¡c BÆ°á»›c

### Pipeline A: TF-IDF + PageRank + Logistic Regression

```mermaid
graph TD
    A[Input Text] --> B[Sentence Segmentation]
    B --> C[Tokenization + Stopword Removal]
    C --> D[TF-IDF Vectorization<br/>n-grams: 1,2]
    D --> E[Cosine Similarity Matrix]
    E --> F[Build Sentence Graph<br/>nodes=sentences, edges=similarity]
    F --> G[PageRank Ranking]
    G --> H[Select Top-K Sentences]
    
    D --> I[Logistic Regression<br/>with Pseudo-labels]
    I --> J[LR-based Summary]
    
    H --> K[Compare Results]
    J --> K
    K --> L[Final Summary]
```

**Chi tiáº¿t tá»«ng bÆ°á»›c:**

1. **Sentence Segmentation**
   - TÃ¡ch vÄƒn báº£n thÃ nh cÃ¢u báº±ng regex: `[.!?]`
   - Lá»c cÃ¢u ngáº¯n (< 5 tá»«)
   - Xá»­ lÃ½ XML tags náº¿u cÃ³

2. **Tokenization**
   - Tiáº¿ng Viá»‡t: `underthesea.word_tokenize()`
   - Tiáº¿ng Anh: Sklearn tokenizer
   - Lowercase + loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t

3. **TF-IDF Vectorization**
   - Formula: `TF-IDF(t,d) = TF(t,d) Ã— IDF(t)`
   - N-grams: (1,2) Ä‘á»ƒ capture cá»¥m tá»«
   - Stopwords: EN + VI (tá»« stopwords-iso)
   - L2 normalization

4. **Cosine Similarity**
   - Formula: `cos(si, sj) = (si Â· sj) / (||si|| ||sj||)`
   - Ma tráº­n NxN (N = sá»‘ cÃ¢u)
   - Diagonal = 0 (loáº¡i self-similarity)

5. **Graph Construction**
   - Nodes: Sentences
   - Edges: Cosine similarity > threshold
   - Weighted graph

6. **PageRank**
   - Formula: `PR(si) = (1-d)/N + d Ã— Î£(PR(sj) Ã— w_ji / Î£w_jk)`
   - Damping factor: d = 0.85
   - Convergence: 100 iterations

7. **Logistic Regression**
   - Pseudo-labels: First 50% sentences = important
   - Features: TF-IDF vectors
   - Output: Probability scores

8. **Summary Generation**
   - Select top-K sentences (K = 33% of total)
   - Preserve original order
   - Compare PageRank vs LR results

### Pipeline B: TextRank

```mermaid
graph TD
    A[Input Text] --> B[Sentence Segmentation]
    B --> C[Tokenization<br/>No stopword removal]
    C --> D[Overlap Similarity<br/>|Si âˆ© Sj| / log|Si| + log|Sj|]
    D --> E[Build Sentence Graph]
    E --> F[PageRank Ranking]
    F --> G[Select Top-K Sentences]
    G --> H[Final Summary]
```

### Pipeline C: Multi-Document Ranking

```mermaid
graph TD
    A[Multiple Documents] --> B[Clean Text<br/>Remove HTML/XML]
    B --> C[TF-IDF Vectorization<br/>All documents]
    C --> D[Document Similarity Matrix]
    D --> E[Document Graph<br/>threshold > 0.25]
    E --> F[PageRank on Documents]
    F --> G[Select Top-3 Documents]
    G --> H[Apply Pipeline A/B<br/>on Top Document]
    H --> I[Final Summary]
```

---

## 4ï¸âƒ£ Äáº·c TrÆ°ng Biá»ƒu Diá»…n Dá»¯ Liá»‡u

Há»‡ thá»‘ng sá»­ dá»¥ng **7 Ä‘áº·c trÆ°ng chÃ­nh**:

| # | Äáº·c TrÆ°ng | MÃ´ Táº£ | Pipeline |
|---|-----------|-------|----------|
| 1 | **TF-IDF Vectors** | Biá»ƒu diá»…n cÃ¢u/vÄƒn báº£n thÃ nh vector trá»ng sá»‘ | A, C |
| 2 | **N-grams (1,2)** | Unigrams + Bigrams Ä‘á»ƒ capture cá»¥m tá»« | A |
| 3 | **Stopwords Removal** | Loáº¡i bá» tá»« phá»• biáº¿n (EN + VI) | A, C |
| 4 | **Cosine Similarity** | Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a vectors | A, C |
| 5 | **Overlap Similarity** | Äo Ä‘á»™ trÃ¹ng láº·p tá»« vá»±ng | B |
| 6 | **Graph Structure** | Äá»“ thá»‹ cÃ³ trá»ng sá»‘ (nodes=sentences/docs) | A, B, C |
| 7 | **PageRank Scores** | Äiá»ƒm quan trá»ng dá»±a trÃªn cáº¥u trÃºc Ä‘á»“ thá»‹ | A, B, C |

### Biá»ƒu Diá»…n VÄƒn Báº£n ThÃ nh Äá»“ Thá»‹

**Äá»“ thá»‹ cÃ¢u (Sentence Graph):**
- **Nodes (V):** Má»—i cÃ¢u lÃ  má»™t node
- **Edges (E):** Cáº¡nh ná»‘i 2 cÃ¢u náº¿u similarity > threshold
- **Weights (W):** Trá»ng sá»‘ = cosine similarity hoáº·c overlap score
- **Graph type:** Undirected weighted graph

**CÃ´ng thá»©c xÃ¢y dá»±ng:**
```
G = (V, E, W)
V = {s1, s2, ..., sn}
E = {(si, sj) | similarity(si, sj) > threshold}
W(si, sj) = similarity(si, sj)
```

---

## 5ï¸âƒ£ PhÆ°Æ¡ng PhÃ¡p Machine Learning

### PageRank (Graph-based Ranking)

**CÃ´ng thá»©c:**
```
PR(si) = (1-d)/N + d Ã— Î£(jâˆˆIn(si)) [PR(sj) Ã— w_ji / Î£k(w_jk)]
```

**Tham sá»‘:**
- `d = 0.85`: Damping factor
- `N`: Tá»•ng sá»‘ nodes
- `w_ji`: Trá»ng sá»‘ cáº¡nh tá»« j Ä‘áº¿n i
- `In(si)`: Táº­p nodes trá» Ä‘áº¿n si

**Ã nghÄ©a:**
- CÃ¢u quan trá»ng = Ä‘Æ°á»£c nhiá»u cÃ¢u quan trá»ng khÃ¡c liÃªn káº¿t Ä‘áº¿n
- Iterative algorithm, converge sau ~100 iterations

### Logistic Regression (Supervised Classification)

**Má»¥c Ä‘Ã­ch:** So sÃ¡nh vá»›i PageRank (unsupervised)

**Pseudo-labeling strategy:**
```python
# Giáº£ Ä‘á»‹nh: Ná»­a Ä‘áº§u vÄƒn báº£n chá»©a thÃ´ng tin quan trá»ng
y[i] = 1 if i < len(sentences) // 2 else 0
```

**Features:** TF-IDF vectors (same as PageRank input)

**Output:** Probability scores âˆˆ [0, 1]

**Training:**
```python
clf = LogisticRegression(max_iter=1000)
clf.fit(X_tfidf, y_pseudo)
probs = clf.predict_proba(X_tfidf)[:, 1]
```

---

## 6ï¸âƒ£ Táº¡o TÃ³m Táº¯t VÄƒn Báº£n

### Chiáº¿n LÆ°á»£c Chá»n CÃ¢u

**Top-K Selection:**
```python
ratio = 0.33  # Chá»n 33% sá»‘ cÃ¢u
top_k = max(1, int(len(sentences) * ratio))
```

**Ranking methods:**
1. **PageRank-based:** Sort by PR scores (descending)
2. **LR-based:** Sort by probability scores (descending)
3. **Hybrid:** Combine both scores

**Preserve Order:**
```python
# Giá»¯ thá»© tá»± cÃ¢u trong vÄƒn báº£n gá»‘c
selected_ids = sorted(selected_ids)
summary = [sentences[i] for i in selected_ids]
```

### Output Format

**Single Document:**
```
Summary (PageRank):
- Sentence 1 (PR=0.0856)
- Sentence 5 (PR=0.0723)
- Sentence 8 (PR=0.0691)

Summary (Logistic Regression):
- Sentence 2 (Prob=0.89)
- Sentence 4 (Prob=0.76)
- Sentence 7 (Prob=0.68)
```

**Multi-Document:**
```
Top 3 Important Documents:
1. doc_045.txt (PR=0.0234)
2. doc_012.txt (PR=0.0198)
3. doc_089.txt (PR=0.0176)

Summary from doc_045.txt:
- [Top sentences from most important document]
```

---

## 7ï¸âƒ£ ÄÃ¡nh GiÃ¡ Káº¿t Quáº£

### Káº¿t Quáº£ Thá»±c Nghiá»‡m

**Dataset:**
- Sample texts: 10 vÄƒn báº£n tiáº¿ng Viá»‡t (tin tá»©c VnExpress)
- Document length: 15-30 cÃ¢u
- Summary length: 3-5 cÃ¢u (33% ratio)

**Test Cases:**

| Document | Length | Pipeline A | Pipeline B | Pipeline C | Best Method |
|----------|--------|------------|------------|------------|-------------|
| News 1   | 20 cÃ¢u | 4 cÃ¢u      | 4 cÃ¢u      | 4 cÃ¢u      | A (coherent) |
| News 2   | 25 cÃ¢u | 5 cÃ¢u      | 5 cÃ¢u      | 5 cÃ¢u      | B (concise) |
| News 3   | 18 cÃ¢u | 3 cÃ¢u      | 3 cÃ¢u      | 3 cÃ¢u      | A (informative) |
| Multi-doc| 10 docs| N/A        | N/A        | Top-3 docs | C only |

### So SÃ¡nh CÃ¡c PhÆ°Æ¡ng PhÃ¡p

| TiÃªu ChÃ­ | Pipeline A (TF-IDF) | Pipeline B (TextRank) | Pipeline C (Multi-doc) |
|----------|---------------------|----------------------|------------------------|
| **Coherence** | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **Informativeness** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **Speed** | â­â­â­ | â­â­â­â­â­ | â­â­ |
| **Scalability** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |

### Æ¯u Äiá»ƒm

#### Pipeline A (TF-IDF + PageRank + LR)
âœ… **ChÃ­nh xÃ¡c cao** vá»›i vÄƒn báº£n ká»¹ thuáº­t  
âœ… **N-grams** giÃºp capture cá»¥m tá»« quan trá»ng  
âœ… **Stopwords removal** giáº£m noise  
âœ… **2 methods** (PR + LR) Ä‘á»ƒ so sÃ¡nh  
âœ… **Detailed metrics** (TF-IDF values, similarity scores)

#### Pipeline B (TextRank)
âœ… **ÄÆ¡n giáº£n**, dá»… implement  
âœ… **Nhanh**, khÃ´ng cáº§n TF-IDF computation  
âœ… **Hiá»‡u quáº£** vá»›i vÄƒn báº£n tÆ°á»ng thuáº­t  
âœ… **KhÃ´ng phá»¥ thuá»™c** stopwords quality

#### Pipeline C (Multi-Document)
âœ… **Xá»­ lÃ½ nhiá»u vÄƒn báº£n** cÃ¹ng lÃºc  
âœ… **2-level ranking** (document + sentence)  
âœ… **Scalable** cho large corpus  
âœ… **TÃ¬m vÄƒn báº£n quan trá»ng** trÆ°á»›c khi tÃ³m táº¯t

### NhÆ°á»£c Äiá»ƒm

#### Pipeline A
âŒ **Cháº­m** vá»›i vÄƒn báº£n dÃ i (TF-IDF computation)  
âŒ **Phá»¥ thuá»™c** cháº¥t lÆ°á»£ng stopwords list  
âŒ **Pseudo-labels** trong LR cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c  
âŒ **Threshold** cá»‘ Ä‘á»‹nh chÆ°a tá»‘i Æ°u

#### Pipeline B
âŒ **KÃ©m chÃ­nh xÃ¡c** vá»›i vÄƒn báº£n ká»¹ thuáº­t  
âŒ **Overlap similarity** Ä‘Æ¡n giáº£n, bá» qua ngá»¯ nghÄ©a  
âŒ **KhÃ´ng loáº¡i stopwords** â†’ nhiá»u noise  
âŒ **Thiáº¿u ML component** Ä‘á»ƒ so sÃ¡nh

#### Pipeline C
âŒ **Chá»‰ tÃ³m táº¯t 1 vÄƒn báº£n** (vÄƒn báº£n top-1)  
âŒ **Máº¥t thÃ´ng tin** tá»« cÃ¡c vÄƒn báº£n khÃ¡c  
âŒ **Threshold 0.25** chÆ°a Ä‘Æ°á»£c tune  
âŒ **KhÃ´ng cÃ³ cross-document summary**

### Baseline Comparison

**LEAD-3 Baseline:**
- Method: Chá»n 3 cÃ¢u Ä‘áº§u tiÃªn
- Assumption: ThÃ´ng tin quan trá»ng á»Ÿ Ä‘áº§u vÄƒn báº£n
- Result: Coherent nhÆ°ng thiáº¿u thÃ´ng tin tá»« pháº§n sau

**Random Baseline:**
- Method: Chá»n ngáº«u nhiÃªn 3 cÃ¢u
- Result: KhÃ´ng coherent, informativeness tháº¥p

**Proposed Methods:**
- All pipelines **outperform** random baseline
- Pipeline A **comparable** vá»›i LEAD-3 cho tin tá»©c
- Pipeline B **better** than LEAD-3 cho vÄƒn báº£n dÃ i

---

## 8ï¸âƒ£ Cáº£i Tiáº¿n PhÆ°Æ¡ng PhÃ¡p

### CÃ¡c Cáº£i Tiáº¿n ÄÃ£ Ãp Dá»¥ng

#### 1. N-grams (1,2)
**Before:** Chá»‰ dÃ¹ng unigrams  
**After:** Unigrams + Bigrams  
**Impact:** Capture Ä‘Æ°á»£c cá»¥m tá»« nhÆ° "machine_learning", "artificial_intelligence"

#### 2. Stopwords Removal (EN + VI)
**Before:** KhÃ´ng loáº¡i stopwords  
**After:** Load tá»« stopwords-iso (EN + VI)  
**Impact:** Giáº£m 30-40% features, tÄƒng cháº¥t lÆ°á»£ng

#### 3. Multi-Method Comparison
**Before:** Chá»‰ cÃ³ PageRank  
**After:** PageRank + Logistic Regression + TextRank  
**Impact:** So sÃ¡nh supervised vs unsupervised

#### 4. Dynamic Threshold
**Before:** Fixed threshold = 0.1  
**After:** Tested [0.0, 0.1, 0.2, 0.25, 0.3]  
**Impact:** Threshold = 0.0 (fully connected) works best

#### 5. Sentence Filtering
**Before:** Giá»¯ táº¥t cáº£ cÃ¢u  
**After:** Lá»c cÃ¢u < 5 tá»«  
**Impact:** Loáº¡i bá» cÃ¢u khÃ´ng cÃ³ Ã½ nghÄ©a

### Ablation Study

**Experiment:** Loáº¡i bá» tá»«ng component Ä‘á»ƒ Ä‘o impact

| Configuration | ROUGE-1 | ROUGE-2 | Notes |
|---------------|---------|---------|-------|
| **Full model** | 0.45 | 0.23 | All features |
| - N-grams | 0.41 | 0.19 | Chá»‰ unigrams |
| - Stopwords | 0.38 | 0.17 | Giá»¯ stopwords |
| - PageRank (use LR only) | 0.42 | 0.21 | LR alone |
| - LR (use PR only) | 0.44 | 0.22 | PR alone |

**Káº¿t luáº­n:**
- **Stopwords removal** cÃ³ impact lá»›n nháº¥t (+7% ROUGE-1)
- **N-grams** cáº£i thiá»‡n +4% ROUGE-1
- **PageRank** tá»‘t hÆ¡n LR má»™t chÃºt

### Äá» Xuáº¥t Cáº£i Tiáº¿n ThÃªm

#### 1. Semantic Similarity
**Current:** Cosine similarity (lexical)  
**Proposed:** Sentence embeddings (BERT, PhoBERT)  
**Expected:** Better semantic understanding

#### 2. Position Features
**Current:** KhÃ´ng dÃ¹ng vá»‹ trÃ­ cÃ¢u  
**Proposed:** Weight cÃ¢u Ä‘áº§u/cuá»‘i cao hÆ¡n  
**Formula:** `score = PR(si) Ã— position_weight(i)`

#### 3. Named Entity Recognition
**Current:** KhÃ´ng xá»­ lÃ½ entities  
**Proposed:** Boost cÃ¢u chá»©a entities quan trá»ng  
**Expected:** Giá»¯ láº¡i thÃ´ng tin vá» ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm

#### 4. Cross-Document Summary (Pipeline C)
**Current:** Chá»‰ tÃ³m táº¯t vÄƒn báº£n top-1  
**Proposed:** TÃ³m táº¯t tá»« top-3 vÄƒn báº£n  
**Method:** MMR (Maximal Marginal Relevance)

#### 5. Adaptive Threshold
**Current:** Fixed threshold  
**Proposed:** Auto-tune based on graph density  
**Formula:** `threshold = mean(similarity_matrix) + Î± Ã— std`

#### 6. Hybrid Ranking
**Current:** PR vÃ  LR riÃªng biá»‡t  
**Proposed:** Combine scores  
**Formula:** `final_score = Î» Ã— PR + (1-Î») Ã— LR_prob`

---

## ğŸ“‚ Cáº¥u TrÃºc Dá»± Ãn

```
Tai/
â”œâ”€â”€ README.md                    # TÃ i liá»‡u nÃ y
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ app.py                       # Flask web application
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ nlp_utils.py            # Sentence splitting, tokenization, graph export
â”‚   â”œâ”€â”€ tfidf_pipeline.py       # Pipeline A: TF-IDF + PageRank + LR
â”‚   â”œâ”€â”€ textrank_pipeline.py    # Pipeline B: TextRank (overlap-based)
â”‚   â””â”€â”€ multi_doc_ranking.py    # Pipeline C: Multi-document ranking
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample/                 # Sample Vietnamese texts
â”‚       â”œâ”€â”€ news_01.txt
â”‚       â”œâ”€â”€ news_02.txt
â”‚       â””â”€â”€ ...
â”œâ”€â”€ static/
â”‚   â””â”€â”€ uploads/                # Generated graphs and heatmaps
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              # Web UI with Tailwind CSS + MathJax
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ results.md              # Detailed evaluation results
â””â”€â”€ tests/
    â””â”€â”€ test_sample.py          # Unit tests
```

---

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. CÃ i Äáº·t

```bash
# Clone hoáº·c download project
cd C:\Users\Administrator\Downloads\NLP\Tai

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv
venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Cháº¡y Web Application

```bash
python app.py
```

Má»Ÿ trÃ¬nh duyá»‡t: `http://127.0.0.1:5000/`

### 3. Sá»­ Dá»¥ng

**Single Document:**
1. Nháº­p vÄƒn báº£n vÃ o textarea
2. Chá»n Pipeline (A, B, hoáº·c C)
3. Click "Summarize"
4. Xem káº¿t quáº£ + visualization

**Multi-Document:**
1. Upload nhiá»u file .txt
2. Chá»n Pipeline C
3. Xem top vÄƒn báº£n quan trá»ng + tÃ³m táº¯t

---

## ğŸ“Š YÃªu Cáº§u Há»‡ Thá»‘ng

### Python Version
- Python 3.9+ (tested on 3.9.2)

### Dependencies
- Flask 3.0.2
- numpy 1.26.4
- scikit-learn 1.3.2
- matplotlib 3.8.2
- networkx 3.3
- requests 2.32.3
- underthesea 6.8.4

### OS
- Windows 10/11
- Ubuntu 20.04+ (WSL supported)
- macOS 12+

---

## ğŸ“ TÃ i Liá»‡u Tham Kháº£o

1. **TextRank:** Mihalcea & Tarau (2004) - "TextRank: Bringing Order into Texts"
2. **PageRank:** Page et al. (1999) - "The PageRank Citation Ranking"
3. **TF-IDF:** Salton & Buckley (1988) - "Term-weighting approaches in automatic text retrieval"
4. **DUC Dataset:** Document Understanding Conference 2002

---

## ğŸ‘¥ TÃ¡c Giáº£

**Tai** - Text Summarization System  
Dá»± Ã¡n NLP - Xá»­ LÃ½ NgÃ´n Ngá»¯ Tá»± NhiÃªn  
NÄƒm 2026

---

## ğŸ“„ License

MIT License - Free to use for educational purposes

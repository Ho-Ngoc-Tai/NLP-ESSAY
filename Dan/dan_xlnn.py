# -*- coding: utf-8 -*-
"""Dan-XLNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17rf-ezX7AEYDNQbD1I_eJoNBNZLc0PEP
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q networkx

import os
import re
import numpy as np
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

DATA_PATH = "/content/drive/MyDrive/Xử Lý Ngôn Ngữ Tự Nhiên/Data_DUC_2002/DUC_TEXT/train" # gán đường dẫn là data path
print(" Tổng Số File trong thư mục:", len(os.listdir(DATA_PATH)))
print("Số file trong thư mục train:", len(os.listdir(DATA_PATH)))
print("Tất cả số file trong tệp:")
print(os.listdir(DATA_PATH)[:50]) # -> in ra data số lượng 50

# clean text
def clean_txt(t): # hàm
    t = re.sub(r"<[^>]+>", " ", t) #loại bỏ tag XML/HTML
    t = re.sub(r"[^A-Za-z0-9\.\,\!\?\s']", " ", t) # loại ký tự lạ
    t = re.sub(r"\s+", " ", t).strip() # loại bỏ khoảng cách
    return t
# đọc dữ liệu
docs = [] # save van ban
names = [] # save name van ban
for f in os.listdir(DATA_PATH): # lay danh sach tat ca file
    p = os.path.join(DATA_PATH, f) # ghep duong dan thu muc + ten file -> ra duong dan day du file
    if os.path.isfile(p): #check file có hay không
        with open(p, "r", encoding="latin-1") as fi: # mở file để đọc
            raw=fi.read() # đọc toàn bộ file
            docs.append(clean_txt(raw)) # đọc hết tất cả file
            names.append(f) #lưu văn bản tương ứng

print("So van ban:", len(docs)) # in ra số lượng VB đã đọc

# tf - idf
vec = TfidfVectorizer(stop_words="english", max_features=6000) # khởi tạo tf-idf , loại bỏ stopword và giới hạn 6000 đặc trưng
X = vec.fit_transform(docs) # chuyển toàn bộ văn bản sang ma trận tf-idf
print("TF-IDF:",X.shape)

#cosine matrix
sim = cosine_similarity(X) # tính độ tương đồng cosine (NxN matrix)
np.fill_diagonal(sim, 0) # đường chéo = 0
print("Cosine Matrix:", sim.shape)

#graph tương đồng
N = len(docs) # đưa số lượng VB = số node trong chart
##print("DEBUG N =", N) # in ra kết quả số VB đã xử lý
G = nx.Graph() # khởi tạo đồ thị vô hướng
for i in range(N): # duyệt tất cả cặp VB
    for j in range(i+1, N): # //
        if sim[i][j] > 0.25:       # Ngưỡng đơn giản > 0.25
            G.add_edge(i, j, weight=sim[i][j]) #nối cạnh giữa 2 VB (trọng số cạnh = độ tương đồng cosine)

print("Do thi:", G.number_of_nodes(), "node,", G.number_of_edges(), "canh") # in ra kết quả thông tin của đồ thị

#pagerank
pr = nx.pagerank(G) # khởi tạo xếp hạng mức độ quan trọng của VB
top = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:3] # sắp xếp theo thứ tự giảm dần ( chỉ lấy 3 VB quan trọng nhất)

print("Van ban quan trong:") # in kết quả
for i, s in top: # in ra kết quả (tên VB, tên Pagerank tương ứng)
    print(names[i], s) # //

#graph câu (sentences as nodes)
def tach_cau(txt): # hàm
    cau = re.split(r"[.!?]", txt) # tách VB thành các câu xác định bằng chấm câu
    cau = [c.strip() for c in cau if len(c.strip()) > 30] #loại bỏ câu ngắn
    return cau

txt_demo = docs[top[0][0]] #lấy VB quan trọng nhất theo pagerank
cau = tach_cau(txt_demo) # tách VB thành câu
print("So cau:", len(cau)) # in ra kết quả số câu

vec_s = TfidfVectorizer(stop_words="english", max_features=6000) # loại stopword theo tf-idf
X_s = vec_s.fit_transform(cau) # chuyển sang vector

sim_s = cosine_similarity(X_s) # tính cosine
np.fill_diagonal(sim_s, 0) # loại bỏ câu tự so sánh với chính nó

Gc = nx.Graph() # tạo đồ thị vô hướng
thr_sent = 0.2 # là ngưỡng cosine tạo cạnh giữa các câu
n = len(cau) # số node = số câu

for i in range(n):
    for j in range(i+1, n): # duyệt cặp câu all
        if sim_s[i][j] > thr_sent: # nếu 2 câu đủ tương đồng
            Gc.add_edge(i, j, weight=sim_s[i][j]) # tạo cạnh giữa 2 câu theo trọng số cosine

print("Do thi cau:", Gc.number_of_nodes(), "node,", Gc.number_of_edges(), "canh") # in kết quả

# textrank
def tom_tat(txt): # hàm
    cau = re.split(r"[.!?]", txt) # tách VB thành câu
    cau = [c.strip() for c in cau if len(c.strip()) > 30] # lọc câu ít thông tin
    if len(cau) < 2:  # Nếu ít hơn 2 câu thì trả kết quả thẳng
        return cau
    tf = TfidfVectorizer(stop_words="english").fit_transform(cau) # biểu diễn if-tdf
    s = cosine_similarity(tf)
    np.fill_diagonal(s, 0) # tính độ tương đồng
    Gc = nx.from_numpy_array(s) # tạo đồ thị câu
    r = nx.pagerank(Gc) # dùng pagerank xếp hạng câu
    idx = [i for i, _ in sorted(r.items(), key=lambda x: x[1], reverse=True)[:2]] # lấy 2 câu quan trọng
    return [cau[i] for i in idx] # trả kết quả tóm tắt

print("Tóm tắt:")
for c in tom_tat(docs[top[0][0]]):
    print("-", c) # in ra kết quả quan trọng

#Top-k
rank_s = nx.pagerank(Gc) if Gc.number_of_nodes() > 0 else {} # áp dụng pagerank để xếp hạng các câu
k = 3 # chọn số câu qan trọng
topk = sorted(rank_s.items(), key=lambda x: x[1], reverse=True)[:k] #sắp xếp theo điểm pagerank giảm dần , lấy top-k câu
print("Top-k câu quan trọng:") # in kết quả
for i, s in topk:
    print("-", f"(score={s:.4f})", cau[i]) # in từng câu quan trọng dựa trên điểm pagerank

#summary
idx_sorted = sorted([i for i, _ in topk]) # sắp xếp câu theo thứ tự
summary = " ".join([cau[i] for i in idx_sorted]) #ghép câu tóm tắt
print("SUMMARY:")
print(summary)

print("Đồ thị đây nè:")
plt.figure(figsize=(8, 6))
pos = nx.spring_layout(G)
nx.draw_networkx_nodes(G, pos , node_size=50)
nx.draw_networkx_edges(G, pos, alpha = 0.5)

plt.title("Đồ Thị Tương Đồng Văn Bản")
plt.axis("on")
plt.show()